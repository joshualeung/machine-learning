{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本笔记中的代码来自 [Recurrent Neural Networks in Tensorflow I](https://r2rt.com/recurrent-neural-networks-in-tensorflow-i.html)。这是一篇介绍 RNN 实现的非常棒的文章，非常清晰易懂，强烈推荐。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports, config variables, and data generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Global config variables\n",
    "num_steps = 5 # number of truncated backprop steps ('n' in the discussion above)\n",
    "batch_size = 200\n",
    "num_classes = 2\n",
    "state_size = 4\n",
    "learning_rate = 0.1\n",
    "\n",
    "def gen_data(size=1000000):\n",
    "    X = np.array(np.random.choice(2, size=(size,)))\n",
    "    Y = []\n",
    "    for i in range(size):\n",
    "        threshold = 0.5\n",
    "        if X[i-3] == 1:\n",
    "            threshold += 0.5\n",
    "        if X[i-8] == 1:\n",
    "            threshold -= 0.25\n",
    "        if np.random.rand() > threshold:\n",
    "            Y.append(0)\n",
    "        else:\n",
    "            Y.append(1)\n",
    "    return X, np.array(Y)\n",
    "\n",
    "# adapted from https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/rnn/ptb/reader.py\n",
    "def gen_batch(raw_data, batch_size, num_steps):\n",
    "    raw_x, raw_y = raw_data\n",
    "    data_length = len(raw_x)\n",
    "\n",
    "    # partition raw data into batches and stack them vertically in a data matrix\n",
    "    batch_partition_length = data_length // batch_size\n",
    "    data_x = np.zeros([batch_size, batch_partition_length], dtype=np.int32)\n",
    "    data_y = np.zeros([batch_size, batch_partition_length], dtype=np.int32)\n",
    "    for i in range(batch_size):\n",
    "        data_x[i] = raw_x[batch_partition_length * i:batch_partition_length * (i + 1)]\n",
    "        data_y[i] = raw_y[batch_partition_length * i:batch_partition_length * (i + 1)]\n",
    "    # further divide batch partitions into num_steps for truncated backprop\n",
    "    epoch_size = batch_partition_length // num_steps\n",
    "\n",
    "    for i in range(epoch_size):\n",
    "        x = data_x[:, i * num_steps:(i + 1) * num_steps]\n",
    "        y = data_y[:, i * num_steps:(i + 1) * num_steps]\n",
    "        yield (x, y)\n",
    "\n",
    "def gen_epochs(n, num_steps):\n",
    "    for i in range(n):\n",
    "        yield gen_batch(gen_data(), batch_size, num_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Placeholders\n",
    "\"\"\"\n",
    "\n",
    "x = tf.placeholder(tf.int32, [batch_size, num_steps], name='input_placeholder')\n",
    "y = tf.placeholder(tf.int32, [batch_size, num_steps], name='labels_placeholder')\n",
    "init_state = tf.zeros([batch_size, state_size])\n",
    "\n",
    "\"\"\"\n",
    "RNN Inputs\n",
    "\"\"\"\n",
    "\n",
    "# Turn our x placeholder into a list of one-hot tensors:\n",
    "# rnn_inputs is a list of num_steps tensors with shape [batch_size, num_classes]\n",
    "x_one_hot = tf.one_hot(x, num_classes)\n",
    "rnn_inputs = tf.unstack(x_one_hot, axis=1)\n",
    "\n",
    "\"\"\"\n",
    "Definition of rnn_cell\n",
    "\n",
    "This is very similar to the __call__ method on Tensorflow's BasicRNNCell. See:\n",
    "https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py#L95\n",
    "\"\"\"\n",
    "with tf.variable_scope('rnn_cell'):\n",
    "    W = tf.get_variable('W', [num_classes + state_size, state_size])\n",
    "    b = tf.get_variable('b', [state_size], initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "def rnn_cell(rnn_input, state):\n",
    "    with tf.variable_scope('rnn_cell', reuse=True):\n",
    "        W = tf.get_variable('W', [num_classes + state_size, state_size])\n",
    "        b = tf.get_variable('b', [state_size], initializer=tf.constant_initializer(0.0))\n",
    "    return tf.tanh(tf.matmul(tf.concat([rnn_input, state], 1), W) + b)\n",
    "\n",
    "\"\"\"\n",
    "Adding rnn_cells to graph\n",
    "\n",
    "This is a simplified version of the \"static_rnn\" function from Tensorflow's api. See:\n",
    "https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/rnn/python/ops/core_rnn.py#L41\n",
    "Note: In practice, using \"dynamic_rnn\" is a better choice that the \"static_rnn\":\n",
    "https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn.py#L390\n",
    "\"\"\"\n",
    "state = init_state\n",
    "rnn_outputs = []\n",
    "for rnn_input in rnn_inputs:\n",
    "    state = rnn_cell(rnn_input, state)\n",
    "    rnn_outputs.append(state)\n",
    "final_state = rnn_outputs[-1]\n",
    "\n",
    "\"\"\"\n",
    "Predictions, loss, training step\n",
    "\n",
    "Losses is similar to the \"sequence_loss\"\n",
    "function from Tensorflow's API, except that here we are using a list of 2D tensors, instead of a 3D tensor. See:\n",
    "https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/seq2seq/python/ops/loss.py#L30\n",
    "\"\"\"\n",
    "\n",
    "#logits and predictions\n",
    "with tf.variable_scope('softmax'):\n",
    "    W = tf.get_variable('W', [state_size, num_classes])\n",
    "    b = tf.get_variable('b', [num_classes], initializer=tf.constant_initializer(0.0))\n",
    "logits = [tf.matmul(rnn_output, W) + b for rnn_output in rnn_outputs]\n",
    "predictions = [tf.nn.softmax(logit) for logit in logits]\n",
    "\n",
    "# Turn our y placeholder into a list of labels\n",
    "y_as_list = tf.unstack(y, num=num_steps, axis=1)\n",
    "\n",
    "#losses and train_step\n",
    "losses = [tf.nn.sparse_softmax_cross_entropy_with_logits(labels=label, logits=logit) for logit, label in zip(logits, y_as_list)]\n",
    "total_loss = tf.reduce_mean(losses)\n",
    "train_step = tf.train.AdagradOptimizer(learning_rate).minimize(total_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Train the network\n",
    "\"\"\"\n",
    "\n",
    "def train_network(num_epochs, num_steps, state_size=4, verbose=True):\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        training_losses = []\n",
    "        for idx, epoch in enumerate(gen_epochs(num_epochs, num_steps)):\n",
    "            training_loss = 0\n",
    "            training_state = np.zeros((batch_size, state_size))\n",
    "            if verbose:\n",
    "                print(\"\\nEPOCH\", idx)\n",
    "            for step, (X, Y) in enumerate(epoch):\n",
    "                tr_losses, training_loss_, training_state, _ = \\\n",
    "                    sess.run([losses,\n",
    "                              total_loss,\n",
    "                              final_state,\n",
    "                              train_step],\n",
    "                                  feed_dict={x:X, y:Y, init_state:training_state})\n",
    "                training_loss += training_loss_\n",
    "                if step % 100 == 0 and step > 0:\n",
    "                    if verbose:\n",
    "                        print(\"Average loss at step\", step,\n",
    "                              \"for last 250 steps:\", training_loss/100)\n",
    "                    training_losses.append(training_loss/100)\n",
    "                    training_loss = 0\n",
    "\n",
    "    return training_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('\\nEPOCH', 0)\n",
      "('Average loss at step', 100, 'for last 250 steps:', 0.60223139226436617)\n",
      "('Average loss at step', 200, 'for last 250 steps:', 0.55299137711524959)\n",
      "('Average loss at step', 300, 'for last 250 steps:', 0.5230247250199318)\n",
      "('Average loss at step', 400, 'for last 250 steps:', 0.52261269330978388)\n",
      "('Average loss at step', 500, 'for last 250 steps:', 0.52028543651103976)\n",
      "('Average loss at step', 600, 'for last 250 steps:', 0.52070176601409912)\n",
      "('Average loss at step', 700, 'for last 250 steps:', 0.5204971581697464)\n",
      "('Average loss at step', 800, 'for last 250 steps:', 0.51935682028532026)\n",
      "('Average loss at step', 900, 'for last 250 steps:', 0.51897449523210526)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x11414ff10>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAD7CAYAAABgzo9kAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nU41wTQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x108d9f950>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_losses = train_network(1,num_steps)\n",
    "plt.plot(training_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_steps = 5, batch_size = 200, num_classes = 2, state_size = 4, learning_rate = 0.1000\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "print(\"num_steps = %d, batch_size = %d, num_classes = %d, state_size = %d, learning_rate = %.4f\" % \n",
    "      (num_steps, batch_size, num_classes, state_size, learning_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of X:  (200, 5)\n",
      "shape of Y:  (200, 5)\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 1\n",
    "for idx, epoch in enumerate(gen_epochs(num_epochs, num_steps)):\n",
    "    if idx > 0:\n",
    "        continue\n",
    "    for step, (X, Y) in enumerate(epoch):\n",
    "        if step > 0:\n",
    "            continue\n",
    "        print(\"shape of X: \", X.shape)\n",
    "        print(\"shape of Y: \", Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据的划分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((200, 5000), (200, 5))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_size = 1000000\n",
    "batch_partition_length = data_size // batch_size # = 1000000 // 200 = 5000\n",
    "epoch_size = batch_partition_length // num_steps # = 5000 // 5 = 1000\n",
    "data_x = np.zeros([batch_size, batch_partition_length], dtype=np.int32)\n",
    "data_x.shape, data_x[:, 1 * num_steps: 2 * num_steps].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">个人理解， 一个 batch 中元素的个数是1000个，因为模型的宽度是5， 所以分成了200行5列。在代码中，epoch_size 实际上是 num_of_batches， 也就是1000。这样命令容易引起误解。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'input_placeholder:0' shape=(200, 5) dtype=int32>,\n",
       " <tf.Tensor 'labels_placeholder:0' shape=(200, 5) dtype=int32>,\n",
       " <tf.Tensor 'zeros:0' shape=(200, 4) dtype=float32>,\n",
       " <tf.Tensor 'one_hot:0' shape=(200, 5, 2) dtype=float32>,\n",
       " 5,\n",
       " <tf.Tensor 'unstack:0' shape=(200, 2) dtype=float32>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y, init_state, x_one_hot, len(rnn_inputs), rnn_inputs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上的输出是对于一个 Graph 来说的。一个 Graph 包含step_size(5)个 Cell, 每个 Cell 的输入是 batch_size * num_classes ( 200 * 2)维度的输入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'concat_13:0' shape=(200, 6) dtype=float32>,\n",
       " <tf.Variable 'rnn_cell/W:0' shape=(6, 4) dtype=float32_ref>,\n",
       " <tf.Variable 'rnn_cell/b:0' shape=(4,) dtype=float32_ref>,\n",
       " <tf.Tensor 'Tanh_7:0' shape=(200, 4) dtype=float32>)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.variable_scope('rnn_cell', reuse=True):\n",
    "        W = tf.get_variable('W', [num_classes + state_size, state_size])\n",
    "        b = tf.get_variable('b', [state_size], initializer=tf.constant_initializer(0.0))\n",
    "new_state = tf.tanh(tf.matmul(tf.concat([rnn_inputs[0], init_state], 1), W) + b)\n",
    "tf.concat([rnn_inputs[0], state], 1), W, b, new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W2 = tf.get_variable('W2', [state_size, num_classes])\n",
    "b2 = tf.get_variable('b2', [num_classes], initializer=tf.constant_initializer(0.0))\n",
    "logits = [tf.matmul(rnn_output, W2) + b2 for rnn_output in rnn_outputs]\n",
    "predictions = [tf.nn.softmax(logit) for logit in logits]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5,\n",
       " <tf.Tensor 'Tanh:0' shape=(200, 4) dtype=float32>,\n",
       " <tf.Variable 'W2:0' shape=(4, 2) dtype=float32_ref>,\n",
       " <tf.Variable 'b2:0' shape=(2,) dtype=float32_ref>,\n",
       " 'logits:',\n",
       " 5,\n",
       " <tf.Tensor 'add_13:0' shape=(200, 2) dtype=float32>,\n",
       " 'predictions:',\n",
       " 5,\n",
       " <tf.Tensor 'Softmax_5:0' shape=(200, 2) dtype=float32>)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rnn_outputs), rnn_outputs[0], W2, b2, \"logits:\", len(logits), logits[0], \"predictions:\", len(predictions), predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'unstack_1:0' shape=(200,) dtype=int32>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_as_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在 run 模型的时候，init_state被设置成了上一次运行结果的返回值。所以`init_state = tf.zeros([batch_size, state_size])`这一行是没有必要的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0.  0.  0.]\n",
      "[ 1.  2.  3.]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sess2 = tf.Session()\n",
    "tmp = tf.zeros([5])\n",
    "print(sess2.run(tmp))\n",
    "tmp = tf.constant([1., 2, 3.])\n",
    "print(sess2.run(tmp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change to Tensorflow API\n",
    "```\n",
    "cell = tf.contrib.rnn.BasicRNNCell(state_size)\n",
    "rnn_outputs, final_state = tf.contrib.rnn.static_rnn(cell, rnn_inputs, initial_state=init_state)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "原文链接: [Recurrent Neural Networks in Tensorflow I](https://r2rt.com/recurrent-neural-networks-in-tensorflow-i.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
